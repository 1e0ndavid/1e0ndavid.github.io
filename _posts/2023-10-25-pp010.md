---
title: "pp010.In-Context Learning Creates Task Vectors"
style: post
tags: paper llm 
---

AI前沿｜ICL底层机理再探

LLM中的上下文学习(ICL)是一种强大的新学习范式。但其潜在机制仍不清楚。特别是，将其映射到“标准”的机器学习框架仍具挑战性，在该框架中，人们使用训练集S在某个假设类中找到最佳拟合函数f(x)。 作者在此通过证明ICL的函数通常具有非常简单的结构：它们对应于transformer型的LLM，其唯一输入是查询x和从训练集计算出来的单个“任务向量”。因此，见figure1，ICL可以看作是将S压缩为单个任务向量θ(S)，然后使用该任务向量来调节transform以产生输出。作者通过一系列模型和任务的综合实验支持了上述主张。

首先是一些背景：为理解ICL的底层机制，即模型如何内在利用实例S和提问x，作者从统计机器学习借用了假设类概念来解决该问题。在学习理论公式中，人们通常考虑假设类H，其中H的每个元素都是函数h(x;θ)，对输入x进行操作，也由参数向量θ决定。例如，如果x ∈ R^d，则类H可以是线性分类器的集合，由系数向量θ定义为h(x;θ)=θ·x。学习算法寻找一个能很好地拟合训练集的元素h ∈ H。这称为经验风险最小化。但我们不清楚是否ICL也在用这种相似的方式运作，因为预测用T([S,x])的形式进行，其中T通常是一个自回归的transformer，方括号里的[S, x]是对S和x中的词的拼接，因此在一般情况下，用来产生输出的可以是任意函数，这也包括非参数方法如最近邻。近期有工作已经在探索该问题，例如当从头开始训练Transformer以在上下文中执行线性回归时，新兴的学习算法类似于随机梯度下降。但对于处理更复杂自然语言任务的LLM来说，我们对假设空间的是啥样的完全没有任何头绪。

作者认为，在广泛的任务中，LLM的ICL可以被视为在非常自然的假设空间上运行。他们认为，给定训练集S，transformer将其映射到“任务向量”θ(S)，该向量本质上表示S中描述的映射/规则。即，给定T和向量θ，我们可以构造实现该任务的新函数f(x;θ)。函数f与应用于x的原始transformer非常相似，但没有了示例，而是通过θ进行调整（见figure2，为使θ独立于提问x，他们用了一个假的提问x'，并且将第L层的箭头的表示作为θ（对中间第几层的选取可以见figure3，作者做了一些比较），然后把这个表示直接放到图右边的相同位置，并且只考虑真实的输入x和箭头作为输入）。作者认为自己的视角和soft prompts相关，两种方法都朝着特定任务调整了transform的函数。但在ICL中，任务向量是在前向传播中计算的而非在微调时。

作者在不同模型不同任务上对假设的验证见figure4。

作者还对任务向量θ进行了直观地展示，其中捕获了有关S演示的任务的信息。他们采用词汇投影方法。结果见table3和7，在多种情况下，他们观察到直接描述任务的标记。重要的是，这些术语从未明确出现在上下文中。例如，在从法语翻译成英语的任务中，他们观察到诸如“English”和“translate”之类的token。这支持了θ携带有关任务的重要的语义信息的观点。
