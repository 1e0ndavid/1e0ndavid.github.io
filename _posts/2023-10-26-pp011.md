---
title: "pp011.DETECTING PRETRAINING DATA FROM LARGE LAN- GUAGE MODELS"
style: post
tags: paper llm 
---

AI前沿｜如何知道某条数据是否在黑盒大语言模型中预训练过


LLM的训练数据很少开源，在多达万亿token的数量中，大概率包含了各种有问题的文本如设计版权信息的、个人信息和基准的测试数据。但当前无法知道某个数据是否被包含在训练数据中及相应比例。作者便研究了预训练数据检测问题：给定一条文本和未知预训练数据的黑盒模型，我们是否能确定该文本被训过。作者为此引入了一个动态基准WIKIMIA，它使用模型训练之前和之后创建的数据来支持检测。他们还引入了一种新的检测方法MIN-K% PROB，该方法基于一个简单的假设：在LLM中，未见过的示例很可能包含一些概率较低的离群词，而见过的示例不太可能包含如此低概率的词。MIN-K% PROB可以在对预训练语料库未知或无任何额外训练的情况下应用，不同于之前需要在与预训练数据相似的数据上训练参考模型的检测方法。此外，实验表明，与之前的方法比，MIN-K% PROB在WIKIMIA上实现了7.4%的改进。作者将MIN-K% PROB应用于三个现实场景：受版权保护的书籍检测、受污染的下游示例检测和机器遗忘的隐私审核，并发现它是一个始终有效的解决方案。比如，从版权书籍检测的实验中可以发现GPT3在从Books3数据中的版权书籍上训练的确凿证据；检测使用机器遗忘过的模型依然会输出相关版权内容。此外，作者对数据集污染检测的研究揭示了预训练设计选择对检测难度的影响，如当训练数据量增加时检测变得更加困难，并且检测示例的出现频率和学习率降低。


MIA，即成员推理攻击作为一种隐私攻击方法，攻击对象是机器学习目标模型，攻击目标是推理一条或一批数据是否作为目标模型的训练集，最先在16年被提出。但采用先前方法的带来两大挑战：首先，与微调不同，预训练使用更大的数据集，每个样例只被训一次，大大降低了被MIA方法记住的潜在可能。此外，先前方法通常依赖于一个或多个以与目标模型相同的方式训练的参考模型（例如，从相同的底层预训练数据分布中采样的影子数据）来实现精确检测。对于LLM来说是不可能的，因为训练分布通常不可用且训练成本太高。


作者首先建立的了基准WIKIMIA，可以定期自动评估任何新发布的预训练LLM。其中通过利用维基百科数据时间戳和模型发布日期，作者选择旧的维基百科事件数据作为成员数据（即在预训练中看过的数据）和最近的维基百科事件数据（如2023年后）作为非成员数据（看不见的）。该数据集有了三个理想的特性：1.准确：保证LLM预训练后发生的事件不会出现在预训练数据中。事件的时间性质确保非成员数据确实看不见，且在预训练数据中未被提及；2.通用：该基准不限于任何特定模型，且可应用于使用维基百科预训练的各种模型（如OPT、LLaMA、GPT-Neo），毕竟维基百科是常用的预训练数据源；3.动态：数据构建流程全自动化，因此可将通过从维基百科收集更新的非成员数据（即更新的事件）来不断更新基准。作者还考虑到用户可能还需要检测经过改述和编辑的文本。不同于只能检测完全匹配的示例的逐字设置之外，作者还引入了改述设置，利用ChatGPT来改述示例，然后评估MIA指标是否能有效识别语义上等效的示例。此外，先前的MIA评估通常在评估中混合不同长度的数据且只汇报一个单一指标。但作者的结果表明，数据长度显著影响检难度。直观上，较短的句子更难检测。因此不同的数据长度桶可能会导致MIA方法的排名不同。作者也因此提出了一个不同长度的设置：将维基百科事件数据截断为不同的长度并分别报告在每个长度上的性能。


考虑到用参考模型做MIA的上述缺点，作者提出一种无需参考模型的MIA方法MIN-K% PROB。该方法基于未见例子中的低概率离群单词不太可能出现在见过的例子中的假设，MIN-K% PROB会计算异常token的平均概率，如图中方程1所示，对于一个给定的序列x，将其中每个xi的对数似然算出来，再将其中的前k%个有最小概率的词挑出来组成一个集合Min-K%(x)，再计算平均值，再将这个平均值和一个阈值做比较，如果超过某个阈值，那么说明数据x很可能已经被训过了。实验表明，该方法使得在WIKIMIA上的AUC评分中比现有最强基线高出7.4%。进一步分析表明，检测性能与模型大小和检测文本长度呈正相关。

主要结果见figure3、4和table2。
