---
title: "pp014.CODEFUSION: A Pre-trained Diffusion Model for Code Generation"
style: post
tags: paper llm 
---

AI前沿｜Codefusion：用于代码生成的预训练扩散模型

微软将扩散模型用在代码生成任务，还透露了ChatGPT参数量

从自然语言生成代码的自回归模型有一个相似的缺陷：它们通常无法容易地重新考虑早先生成的token，作者引入了CODEFUSION，这是一种预先训练的扩散代码生成模型，它通过迭代地对以编码自然语言为条件的完整程序进行去噪来解决上述问题。作者针对Bash、Python和Microsoft Excel条件格式 (CF) 规则的自然语言到代码生成任务评估CODEFUSION，实验表明，CODEFUSION（75M参数）在top-1精度方面与sota的自回归系统（350M–175B参数）相当，并且在top-3和top-5精度方面优于它们，因为它能更好地平衡多样性与质量。

不同于利用解码策略来平衡生成文本的多样性与质量，作者提出将扩散模型这一在图像生成领域有优越表现的技术用在代码生成上。在先前将扩散用在文本生成上的工作中，它们通常利用一个嵌入层将离散token转换成连续嵌入，其中高斯噪声能被加入或预测以模仿扩散过程。在将去噪的嵌入映射回离散文本时，这些方法会选择最相近的嵌入对应的词，但在代码领域通常有很多句法语义限制，独立地将嵌入投影回token可能会产生无效的程序。因此作者提出Codefusion，见figure1，将编解码架构与扩散过程结合，编码器将NL映射成连续表示，被扩散模型用作额外的条件用于对随机高斯噪声输入进行去噪。为生成句法的正确代码，接着将去噪了的嵌入输入到解码器，经过全自注意力和与嵌入语句的交叉注意，取得代码token的概率分布，再选取。

为了预训练Codefusion生成代码，他们扩展连续段落去噪（CPD）任务到代码领域。具体来说，他们只将噪声应用于与代码中的识别符或目标语言中的内置关键字上，这使得模型能学会关键代码token间的关系（如变量名、函数名和控制流内置变量）

见table2，codefusion相比自回归模型产生了更多样性的代码（更高n-gram比例、更低嵌入相似度与更高的编辑距离）。而CPD目标让模型更偏好以上下文感知的方式消除噪音，搭配可以访问完整降噪后的表示的解码器，使得新模型相比GENIE这一文本扩散模型产生语法正确率提高48.5%的结果，见table3。

在Python，Bash和CF规则上的结果见table1，效果还是很好的。另外微软的作者还透露了ChatGPT的参数量是20B。
