---
title: "pp020.Orca 2: Teaching Small Language Models  How to Reason"
style: post
tags: paper llm 
---

AI｜Orca 2：利用提示擦除技术让模型知道如何选择回复策略

本文是Orca 1的续作，Orca 1中通过在数据中加入更丰富的信号如对过程的解释（Explanation Tuning）使得模型在像BigBench Hard和AGIEval上得到高分。在此，作者继续探索增强的训练信号如何提高小型LM的推理能力。训练小型LM的研究通常依赖于模仿学习来复制能力更强的模型的输出。作者认为，过度强调模仿可能会限制小模型的潜力。他们试图教导小模型针对不同的任务采用不同的解决策略，这些策略可能与较大模型使用的策略不同。例如，虽然较大的模型直接为复杂任务给出答案，但较小的模型可能不具备相同能力。在Orca 2中，作者教授模型各种推理技术（逐步式、回忆然后生成、回忆-推理-生成、直接回答等）。更重要的是，作者的目标是帮助模型学习确定每项任务最有效的解决策略。他们使用一套含15个不同基准的基准（对应于大约100个任务和超过36,000个非重复的提示）来评估Orca 2。Orca 2显著超越了类似尺寸的模型，见figure1，在零样本情形下测试高级推理能力的复杂任务上进行的评估，达到了与5-10倍大的模型相似或更好的性能水平。作者开源了Orca 2源码以鼓励对小型LM的开发、评估和进一步研究。

Orca 2的目标有两个。首先是教较小的模型如何使用一套推理技术，例如逐步处理、回忆然后生成、回忆-推理-生成、抽取-生成和直接回答方法。其次，作者希望帮助这些模型决定何时对手头的任务使用最有效的推理策略，使它们能够发挥最佳性能，无论规模如何。

像Orca 1一样，作者用更强的LLM来展示给小模型跨各种任务的各种推理策略。但在Orca 2中，推理策略是根据当前任务精心定制的，还同时考虑到学生模型是否有足够能力做出相似回答。作者认为Explanation Tuning的关键是根据系统指令从LLM中提取带有详细解释的答案。但并非每种系统指令的组合都是合适的，回复质量可能会根据系统指令中描述的策略而变化。就算GPT4也很容易被系统指令的变化影响，如figure3中GPT4在四种不同系统指令下针对故事重排序问题给出的回复，其中：1这个默认答案是错的；2用了CoT提示，答案更好；3用了解释答案的提示，回复也错了但解释正确；4是唯一正确的答案，用的是下图的系统指令。可以看出来模型受系统指令影响很大且当系统指令好好构造的时候能大大提高效果。所以系统指令的质量非常重要。作者用Cautious Reasoning来指代在解答时选取何种解码策略的行为。训练一个Cautions Reasoning模型的步骤如下，非常清晰：

1. 收集各种不同的任务；
2. 由Orca效果的指导下，决定何种任务该用何种解答策略；
3. 构造与选定的策略相关的任务特定的系统指令来取得老师模型对于每个任务的回复；
4. 提示擦除Prompt Erasure：训练时将学生系统指令替换成一个通用的系统指令，其中不包含如何完成任务的细节。

另外作者认为先前的对模型的评估不够全面或让其他模型自动评估不够好，他们便在此提供了全面综合的评估，覆盖约100个任务。
